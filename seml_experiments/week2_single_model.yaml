# Experiment configuration file.
#
# There are two special blocks. The 'seml' block is required for every experiment.
# It has to contain the following values:
# executable:        Name of the Python script containing the experiment. The path should be relative to the `project_root_dir`.
#                    For backward compatibility SEML also supports paths relative to the location of the config file.
#                    In case there are files present both relative to the project root and the config file,
#                    the former takes precedence.
# It can optionally also contain the following values:
# name:              Prefix for output file and Slurm job name. Default: Collection name
# output_dir:        Directory to store log files in. Default: Current directory
# conda_environment: Specifies which Anaconda virtual environment will be activated before the experiment is executed.
#                    Default: The environment used when queuing.
# project_root_dir:  (Relative or absolute) path to the root of the project. seml will then upload all the source
#                    files imported by the experiment to the MongoDB. Moreover, the uploaded source files will be
#                    downloaded before starting an experiment, so any changes to the source files in the project
#                    between queueing and starting the experiment will have no effect.
#
# The special 'slurm' block contains the slurm parameters. This block and all values are optional. Possible values are:
# experiments_per_job:     Number of parallel experiments to run in each Slurm job.
#                          Note that only experiments from the same batch share a job. Default: 1
# max_simultaneous_jobs:   Maximum number of simultaneously running Slurm jobs per job array. Default: No restriction
# sbatch_options_template: Name of a custom template of `SBATCH` options. Define your own templates in `settings.py`
#                          under `SBATCH_OPTIONS_TEMPLATES`, e.g. for long-running jobs, CPU-only jobs, etc.
# sbatch_options:          dictionary that contains custom values that will be passed to `sbatch`, specifying e.g.
#                          the memory and number of GPUs to be allocated (prepended dashes are not required). See
#                          https://slurm.schedmd.com/sbatch.html for all possible options.
#
# Parameters under 'fixed' will be used for all the experiments.
#
# Under 'grid' you can define parameters that should be sampled from a regular grid. Options are:
#   - choice:     List the different values you want to evaluate under 'choices' as in the example below.
#   - range:      Specify the min, max, and step. Parameter values will be generated using np.arange(min, max, step).
#   - uniform:    Specify the min, max, and num. Parameter values will be generated using
#                 np.linspace(min, max, num, endpoint=True)
#   - loguniform: Specify min, max, and num. Parameter values will be uniformly generated in log space (base 10).
#
# Under 'random' you can specify parameters for which you want to try several random values. Specify the number
# of samples per parameter with the 'samples' value as in the examples below.
# Specify the the seed under the 'random' dict or directly for the desired parameter(s).
# Supported parameter types are:
#   - choice:      Randomly samples <samples> entries (with replacement) from the list in parameter['options']
#   - uniform:     Uniformly samples between 'min' and 'max' as specified in the parameter dict.
#   - loguniform:  Uniformly samples in log space between 'min' and 'max' as specified in the parameter dict.
#   - randint:     Randomly samples integers between 'min' (included) and 'max' (excluded).
#
# The configuration file can be nested (as the example below) so that we can run different parameter sets
# e.g. for different datasets or models.
# We take the cartesian product of all `grid` parameters on a path and sample all random parameters on the path.
# The number of random parameters sampled will be max{n_samples} of all n_samples on the path. This is done because
# we need the same number of samples from all random parameters in a configuration.
#
# More specific settings (i.e., further down the hierarchy) always overwrite more general ones.


seml:
  executable: training_semi_supervised_node_classification.py
  name: week2_single_model
  output_dir: /nfs/students/fuchsgru/seml_output/week2_single_model
  project_root_dir: ..

slurm:
  experiments_per_job: 1
  sbatch_options:
    gres: gpu:1       # num GPUs
    mem: 16G          # memory
    cpus-per-task: 2  # num cores
    time: 0-08:00     # max time, D-HH:MM


###### BEGIN PARAMETER CONFIGURATION ######

fixed:
  training.early_stopping.patience: 100
  training.early_stopping.mode: min
  training.early_stopping.monitor: val_loss
  training.early_stopping.min_delta: 1e-3
  training.max_epochs: 1000
  training.gpus: 1
  data.num_dataset_splits : 1
  data.train_portion : 0.05
  data.val_portion : 0.15
  data.test_portion : 0.6
  data.test_portion_fixed : 0.2
  model.weight_scale : 5
  model.num_initializations : 1
  model.use_bias: true
  model.activation: leaky_relu
  model.leaky_relu_slope: 0.01

grid:
  model.use_spectral_norm:
      type: choice
      options:
        - true
        - false

cora_ml-gat:
  fixed:
    data.dataset: cora_ml
    model.model_type: gat
    training.learning_rate: 0.003
    model.hidden_sizes: [64, 64]
    model.num_heads: 8


cora_ml-gcn:
  fixed:
    data.dataset: cora_ml
    model.model_type: gcn
    training.learning_rate: 0.003
    model.hidden_sizes: [64]


cora_ml-gin:
  fixed:
    data.dataset: cora_ml
    model.model_type: gin
    training.learning_rate: 0.003
    model.hidden_sizes: [64]


cora_ml-sage:
  fixed:
    data.dataset: cora_ml
    model.model_type: sage
    training.learning_rate: 0.003
    model.hidden_sizes: [64]


cora_ml-mlp:
  fixed:
    data.dataset: cora_ml
    model.model_type: mlp
    training.learning_rate: 0.001
    model.hidden_sizes: [32]


cora_ml-appnp:
  fixed:
    data.dataset: cora_ml
    model.model_type: appnp
    training.learning_rate: 0.003
    model.hidden_sizes: [64, 64]
    model.teleportation_probability: 0.1
    model.diffusion_iterations: 10


citeseer-gat:
  fixed:
    data.dataset: citeseer
    model.model_type: gat
    training.learning_rate: 0.001
    model.hidden_sizes: [64, 64]
    model.num_heads: 8


citeseer-gcn:
  fixed:
    data.dataset: citeseer
    model.model_type: gcn
    training.learning_rate: 0.010
    model.hidden_sizes: [64]


citeseer-gin:
  fixed:
    data.dataset: citeseer
    model.model_type: gin
    training.learning_rate: 0.010
    model.hidden_sizes: [64]


citeseer-sage:
  fixed:
    data.dataset: citeseer
    model.model_type: sage
    training.learning_rate: 0.010
    model.hidden_sizes: [64]


citeseer-mlp:
  fixed:
    data.dataset: citeseer
    model.model_type: mlp
    training.learning_rate: 0.010
    model.hidden_sizes: [64]


citeseer-appnp:
  fixed:
    data.dataset: citeseer
    model.model_type: appnp
    training.learning_rate: 0.003
    model.hidden_sizes: [32]
    model.teleportation_probability: 0.1
    model.diffusion_iterations: 10


pubmed-gat:
  fixed:
    data.dataset: pubmed
    model.model_type: gat
    training.learning_rate: 0.010
    model.hidden_sizes: [64, 64]
    model.num_heads: 8


pubmed-gcn:
  fixed:
    data.dataset: pubmed
    model.model_type: gcn
    training.learning_rate: 0.003
    model.hidden_sizes: [64, 64]


pubmed-gin:
  fixed:
    data.dataset: pubmed
    model.model_type: gin
    training.learning_rate: 0.003
    model.hidden_sizes: [64]


pubmed-sage:
  fixed:
    data.dataset: pubmed
    model.model_type: sage
    training.learning_rate: 0.010
    model.hidden_sizes: [64]


pubmed-mlp:
  fixed:
    data.dataset: pubmed
    model.model_type: mlp
    training.learning_rate: 0.003
    model.hidden_sizes: [64, 64]


pubmed-appnp:
  fixed:
    data.dataset: pubmed
    model.model_type: appnp
    training.learning_rate: 0.003
    model.hidden_sizes: [64, 64]
    model.teleportation_probability: 0.1
    model.diffusion_iterations: 2
